import os
import csv
from dotenv import load_dotenv
from langchain_neo4j import Neo4jGraph
from langchain_google_genai import ChatGoogleGenerativeAI # Import for Gemini LLM
from langchain.prompts import PromptTemplate
# from langchain.chains import LLMChain # Removed as LLMChain is being replaced

# Load environment variables from .env file
load_dotenv()

# --- Neo4j Connection Details ---
NEO4J_URI = os.getenv("NEO4J_URI", "bolt://localhost:7687")
NEO4J_USERNAME = os.getenv("NEO4J_USERNAME", "neo4j")
NEO4J_PASSWORD = os.getenv("NEO4J_PASSWORD", "password") # Replace with your actual password or use env var

# --- Path to your local CSV file (Python will read this) ---
# Ensure this path is correct for your local file system.
SPOTIFY_HISTORY_CSV_PATH = 'dataset/spotify_history.csv'

# --- Batch Size for Processing ---
# This determines how many rows are sent to Neo4j in one transaction.
# Adjust based on memory and performance needs.
BATCH_SIZE = 1000

def load_spotify_data_into_neo4j_with_llm_generated_query():
    """
    Orchestrates the loading of Spotify history data into Neo4j
    by reading CSV in Python and sending batched queries,
    with the core Cypher logic generated by an LLM.
    """
    print("Attempting to connect to Neo4j...")
    try:
        # Initialize LangChain's Neo4jGraph for schema interactions and general queries.
        # Ensure APOC is installed and configured in your Neo4j instance as it might be used internally.
        graph = Neo4jGraph(
            url=NEO4J_URI,
            username=NEO4J_USERNAME,
            password=NEO4J_PASSWORD
        )
        print("Successfully connected to Neo4j.")

        # --- Cypher Queries for Schema Definition (still relevant for initial setup) ---
        # Dropping existing constraints by their names with IF EXISTS, each in a separate query.
        print("\nDropping existing constraints (if any)...")
        graph.query("DROP CONSTRAINT TrackUriUnique IF EXISTS;")
        graph.query("DROP CONSTRAINT ArtistNameUnique IF EXISTS;")
        graph.query("DROP CONSTRAINT AlbumNameUnique IF EXISTS;")
        print("Existing constraints dropped.")

        # Creating new unique constraints with explicit names, each in a separate query.
        print("\nCreating new unique constraints...")
        graph.query("CREATE CONSTRAINT TrackUriUnique IF NOT EXISTS FOR (t:Track) REQUIRE t.spotify_track_uri IS UNIQUE;")
        graph.query("CREATE CONSTRAINT ArtistNameUnique IF NOT EXISTS FOR (a:Artist) REQUIRE a.name IS UNIQUE;")
        graph.query("CREATE CONSTRAINT AlbumNameUnique IF NOT EXISTS FOR (al:Album) REQUIRE al.name IS UNIQUE;")
        print("Unique constraints created.")

        # --- LLM for Cypher Query Generation ---
        # Initialize the LLM (Gemini in this case)
        # temperature=0.0 makes the output more deterministic.
        # Ensure GOOGLE_API_KEY environment variable is set.
        llm = ChatGoogleGenerativeAI(model="gemini-2.5-flash", temperature=0.0, api_key=os.getenv("GEMINI_API_KEY"))

        # Define the schema and CSV columns for the LLM's context.
        # This information guides the LLM to generate the correct Cypher.
        csv_columns_info = """
        The CSV file has the following columns:
        - spotify_track_uri: Unique identifier for each track (e.g., "spotify:track:<base-62 string>")
        - ts: Timestamp indicating when the track stopped playing in UTC (Coordinated Universal Time)
        - platform: Platform used when streaming the track
        - ms_played: Number of milliseconds the stream was played
        - track_name: Name of the track
        - artist_name: Name of the artist
        - album_name: Name of the album
        - reason_start: Why the track started
        - reason_end: Why the track ended
        - shuffle: TRUE or FALSE depending on if shuffle mode was used
        - skipped: TRUE or FALSE depending on if the user skipped
        """
        graph_schema_description = """
        You need to create nodes and relationships based on this schema:
        Nodes:
        - :Track {spotify_track_uri: String (Unique ID), name: String}
        - :Artist {name: String (Unique ID)}
        - :Album {name: String (Unique ID)}
        - :PlaySession {timestamp: Integer, ms_played: Integer, platform: String,
                         reason_start: String, reason_end: String, shuffle: Boolean, skipped: Boolean}

        Relationships:
        - (:PlaySession)-[:PLAYED_TRACK]->(:Track)
        - (:PlaySession)-[:PLAYED_ARTIST]->(:Artist)
        - (:PlaySession)-[:PLAYED_ALBUM]->(:Album)
        - (:Track)-[:BELONGS_TO_ALBUM]->(:Album)
        - (:Track)-[:PERFORMED_BY]->(:Artist)
        """

        # Prompt template to instruct the LLM for Cypher generation.
        # We ask for the core MERGE/CREATE logic for a single 'row' (a map).
        prompt_template = PromptTemplate(
            input_variables=["csv_columns_info", "graph_schema_description"],
            template="""
            You are a Neo4j Cypher query generator.
            Given the input 'row' which is a map (dictionary) derived from a CSV row.
            The CSV columns are:
            {csv_columns_info}

            The desired Neo4j graph schema is:
            {graph_schema_description}

            Generate the Cypher query snippet (MERGE/CREATE statements) that processes a single 'row' map.
            Ensure correct data type conversions (e.g., toInteger(), toBoolean(), toFloat()).
            Do NOT include LOAD CSV, UNWIND, or any transaction control statements.
            Just generate the MERGE/CREATE logic for one 'row'.
            Assume 'row.<column_name>' syntax for accessing properties.

            Cypher snippet (Do not include code block wrapping):
            """
        )

        print("\nGenerating Cypher query snippet with LLM...")
        # Use invoke directly on the LLM with the formatted prompt
        # The invoke method returns a BaseMessage object, so we access its content.
        llm_generated_cypher_snippet = llm.invoke(
            prompt_template.format_prompt(
                csv_columns_info=csv_columns_info,
                graph_schema_description=graph_schema_description
            )
        ).content # Access the content of the BaseMessage object

        print("LLM generated Cypher snippet:")
        print(llm_generated_cypher_snippet)

        # Construct the full batched query using the LLM-generated snippet.
        # The UNWIND part handles iterating over the batch of rows.
        load_batch_query = f"""
        UNWIND $rows AS row
        {llm_generated_cypher_snippet}
        """

        print(f"\nStarting to load data from {SPOTIFY_HISTORY_CSV_PATH} in batches...")
        processed_rows = 0
        batch = []

        with open(SPOTIFY_HISTORY_CSV_PATH, 'r', encoding='utf-8-sig') as file:
            reader = csv.DictReader(file)
            for row_data in reader: # Renamed 'row' to 'row_data' to avoid conflict with Cypher 'row' variable.
                # Append the row to the current batch
                print(row_data)
                batch.append(row_data)
                processed_rows += 1

                # If the batch is full, execute the query
                if len(batch) >= BATCH_SIZE:
                    print(f"Processing batch of {len(batch)} rows (Total processed: {processed_rows - len(batch)} to {processed_rows})...")
                    # Pass the list of row dictionaries under the key "rows"
                    graph.query(load_batch_query, {"rows": batch})
                    batch = [] # Clear the batch

        # Process any remaining rows in the last batch
        if batch:
            print(f"Processing final batch of {len(batch)} rows (Total processed: {processed_rows - len(batch)} to {processed_rows})...")
            graph.query(load_batch_query, {"rows": batch})

        print("Data loading complete!")

        # Optional: Verify data count
        count_query = """
        MATCH (n) RETURN count(n) AS count, 'Nodes' AS entityType
        UNION ALL
        MATCH ()-[r]->() RETURN count(r) AS count, 'Relationships' AS entityType;
        """
        results = graph.query(count_query)
        print("\nData verification:")
        for record in results:
            if 'nodeCount' in record:
                print(f"Total Nodes: {record['nodeCount']}")
            if 'relationshipCount' in record:
                print(f"Total Relationships: {record['relationshipCount']}")

    except Exception as e:
        print(f"An error occurred: {e}")

if __name__ == "__main__":
    load_spotify_data_into_neo4j_with_llm_generated_query()